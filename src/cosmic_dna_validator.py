#Voici le code complet, fusionn√© et corrig√©, incluant toutes les fonctionnalit√©s du projet Cosmic DNA : g√©n√©ration de catalogues avec CAMB, √©volution temporelle via Leapfrog, optimisation MCMC, comparaison avec IllustrisTNG, et m√©triques suppl√©mentaires (rapport vide-amas, p√©riodicit√©). La m√©thode compute_covariance_matrix est mise √† jour pour utiliser 500 r√©√©chantillons bootstrap, avec des messages de progression ajust√©s pour une ex√©cution robuste.

# src/cosmic_dna_validator.py

import numpy as np
from scipy import stats
from scipy.spatial import cKDTree
from scipy.interpolate import interp1d
from scipy.linalg import pinv
from astropy.cosmology import Planck18
from sklearn.cluster import DBSCAN
import camb
import emcee
import warnings
warnings.filterwarnings('ignore')

class RedshiftSpaceValidator:
    """
    Validateur cosmologique de base avec distorsions de redshift et correction Kaiser
    """
    
    def __init__(self, cosmology=Planck18):
        self.cosmo = cosmology
        self.H0 = cosmology.H0.value  # km/s/Mpc
        self.Omega_m = cosmology.Om0
        
        # Donn√©es SDSS de r√©f√©rence (Zehavi et al. 2011 - espace de redshift)
        self.sdss_s = np.array([0.1, 0.5, 1, 5, 10, 20, 50])  # s en Mpc/h
        self.sdss_xi_s = np.array([18, 7.5, 3.8, 1.2, 0.65, 0.25, 0.08])  # Œæ(s)
        self.sdss_errors = np.array([2.0, 0.5, 0.3, 0.1, 0.05, 0.03, 0.01])
        
        print(f"‚úÖ Cosmologie initialis√©e: H0 = {self.H0:.1f}, Œ©m = {self.Omega_m:.3f}")
    
    def generate_realistic_catalog_with_velocities(self, n_galaxies=15000, box_size=500, 
                                                 bias=1.8, sigma_v=400, random_seed=42):
        """
        G√©n√®re un catalogue r√©aliste avec vitesses propres
        """
        if random_seed is not None:
            np.random.seed(random_seed)
            
        print(f"üîÑ G√©n√©ration catalogue avec vitesses: {n_galaxies} galaxies...")
        
        grid_size = 128
        density_field, k_vectors = self._generate_density_field_with_gradients(grid_size, box_size)
        biased_density = self._apply_galaxy_bias(density_field, bias)
        positions = self._sample_positions_from_density(biased_density, n_galaxies, box_size)
        velocities = self._compute_peculiar_velocities(positions, density_field, box_size, sigma_v, bias)
        
        print(f"‚úÖ Catalogue g√©n√©r√©: {len(positions)} galaxies")
        print(f"   - Biais: {bias}, œÉ_v: {sigma_v} km/s")
        print(f"   - Bo√Æte: {box_size} Mpc/h")
        
        return positions, velocities, box_size
    
    def _generate_density_field_with_gradients(self, grid_size, box_size):
        """G√©n√®re un champ de densit√© avec CAMB"""
        print("üîÑ G√©n√©ration champ de densit√© avec CAMB...")
        pars = camb.CAMBparams()
        pars.set_cosmology(H0=self.H0, ombh2=0.0224, omch2=0.120)
        pars.InitPower.set_params(ns=0.965)
        pars.set_matter_power(redshifts=[0.0], kmax=2.0)
        results = camb.get_results(pars)
        k, _, Pk = results.get_matter_power_spectrum(minkh=1e-4, maxkh=1.0, npoints=grid_size)
        
        kx = np.fft.fftfreq(grid_size) * 2 * np.pi * grid_size / box_size
        ky = np.fft.fftfreq(grid_size) * 2 * np.pi * grid_size / box_size
        kz = np.fft.fftfreq(grid_size) * 2 * np.pi * grid_size / box_size
        KX, KY, KZ = np.meshgrid(kx, ky, kz, indexing='ij')
        k_mag = np.sqrt(KX**2 + KY**2 + KZ**2)
        k_mag[0, 0, 0] = 1e-10
        
        Pk_interp = interp1d(k, Pk[0], bounds_error=False, fill_value=0)
        Pk_grid = Pk_interp(k_mag)
        
        phase = np.random.normal(0, 1, (grid_size, grid_size, grid_size)) + \
                1j * np.random.normal(0, 1, (grid_size, grid_size, grid_size))
        field_fourier = np.sqrt(Pk_grid) * phase
        density_field = np.real(np.fft.ifftn(field_fourier))
        density_field = (density_field - np.min(density_field)) / \
                       (np.max(density_field) - np.min(density_field))
        
        return density_field, (KX, KY, KZ)
    
    def _apply_galaxy_bias(self, density_field, bias):
        """Applique le biais des galaxies"""
        return np.maximum(bias * density_field + 0.2 * bias * density_field**2, 0)
    
    def _sample_positions_from_density(self, density_field, n_galaxies, box_size):
        """√âchantillonne les positions selon la densit√©"""
        grid_size = density_field.shape[0]
        flat_density = density_field.flatten()
        probabilities = flat_density / np.sum(flat_density)
        
        cell_indices = np.random.choice(len(probabilities), size=n_galaxies, p=probabilities)
        cell_coords = np.unravel_index(cell_indices, (grid_size, grid_size, grid_size))
        
        positions = np.column_stack([
            (cell_coords[0] + np.random.random(n_galaxies)) * box_size / grid_size,
            (cell_coords[1] + np.random.random(n_galaxies)) * box_size / grid_size,
            (cell_coords[2] + np.random.random(n_galaxies)) * box_size / grid_size
        ])
        
        return positions
    
    def _compute_peculiar_velocities(self, positions, density_field, box_size, sigma_v, bias):
        """Calcule les vitesses propres"""
        n_gal = len(positions)
        grid_size = density_field.shape[0]
        f = self.Omega_m**0.55
        
        velocities = np.random.normal(0, sigma_v, (n_gal, 3)) * f / bias
        cell_size = box_size / grid_size
        for i, pos in enumerate(positions):
            cell_x = int(pos[0] / cell_size) % grid_size
            cell_y = int(pos[1] / cell_size) % grid_size
            cell_z = int(pos[2] / cell_size) % grid_size
            local_density = density_field[cell_x, cell_y, cell_z]
            velocities[i] *= (1.0 + 2.0 * local_density)
        
        return velocities
    
    def apply_redshift_space_distortions(self, positions, velocities, los_direction=None):
        """Applique les distorsions de redshift"""
        if los_direction is None:
            los_direction = np.array([0, 0, 1])
        los_direction = los_direction / np.linalg.norm(los_direction)
        v_los = np.dot(velocities, los_direction)
        s_displacement = v_los / self.H0
        redshift_positions = positions + s_displacement[:, np.newaxis] * los_direction
        return redshift_positions
    
    def kaiser_correction_model(self, xi_real, beta, r_values):
        """Mod√®le de Kaiser pour Œæ(s)"""
        beta = self.Omega_m**0.55 / beta
        kaiser_factor = 1 + (2/3) * beta + (1/5) * beta**2
        finger_of_god = np.exp(-(r_values/5.0)**2)
        return xi_real * kaiser_factor * finger_of_god
    
    def compute_correlation_function(self, positions, r_bins, box_size, space='real'):
        """Calcule Œæ(r) ou Œæ(s) avec estimateur Landy-Szalay"""
        n_gal = len(positions)
        n_random = min(10000, n_gal * 2)
        random_positions = np.random.uniform(0, box_size, (n_random, 3))
        
        DD = self._count_pairs(positions, positions, r_bins, box_size)
        DR = self._count_pairs(positions, random_positions, r_bins, box_size)
        RR = self._count_pairs(random_positions, random_positions, r_bins, box_size)
        
        RR_safe = np.where(RR > 1e-10, RR, 1e-10)
        xi = (DD - 2 * DR + RR) / RR_safe
        r_centers = r_bins[:-1] + np.diff(r_bins)/2
        return r_centers, xi, space
    
    def _count_pairs(self, pos1, pos2, r_bins, box_size):
        """Comptage de paires optimis√©"""
        tree1 = cKDTree(pos1, boxsize=box_size)
        tree2 = cKDTree(pos2, boxsize=box_size)
        counts = np.zeros(len(r_bins) - 1)
        for i in range(len(r_bins) - 1):
            counts[i] = tree1.count_neighbors(tree2, r_bins[i+1], r_bins[i])
        return counts
    
    def compute_power_spectrum(self, positions, box_size, k_bins=20):
        """Calcule P(k)"""
        grid_size = 128
        delta, _ = np.histogramdd(positions, bins=grid_size, range=[[0, box_size]]*3)
        delta /= np.mean(delta) - 1
        delta_k = np.fft.fftn(delta)
        kx = np.fft.fftfreq(grid_size) * 2 * np.pi * grid_size / box_size
        k_bins_edges = np.logspace(-2, 0, k_bins + 1)
        k_centers = k_bins_edges[:-1] + np.diff(k_bins_edges)/2
        Pk = np.zeros(k_bins)
        for i in range(k_bins):
            mask = (kx >= k_bins_edges[i]) & (kx < k_bins_edges[i+1])
            Pk[i] = np.mean(np.abs(delta_k[mask])**2)
        return k_centers, Pk

class CosmicDNALeapfrogValidator(RedshiftSpaceValidator):
    """
    Validateur avec int√©gration Leapfrog pour l'√©volution temporelle
    """
    
    def evolve_positions_leapfrog(self, positions, velocities, box_size, dt=0.01, 
                                 alpha=5e-17, beta=5e-17, gamma=5e-17, delta=5e-17, 
                                 nu=5e-17, b=5e-17, c=5e-17, n_steps=10):
        """√âvolue les positions avec Leapfrog"""
        print(f"üîÑ √âvolution Leapfrog: {n_steps} pas...")
        positions = positions.copy()
        velocities = velocities.copy()
        density_field, _ = self._generate_density_field_with_gradients(128, box_size)
        cell_size = box_size / 128
        
        for step in range(n_steps):
            accel = np.zeros_like(positions)
            for i in range(len(positions)):
                for j in range(i + 1, len(positions)):
                    r_vec = positions[j] - positions[i]
                    r = np.linalg.norm(r_vec)
                    if r > 1e-3:
                        accel[i] += alpha * r_vec / (r**3 + 1e-3)
                        accel[j] -= alpha * r_vec / (r**3 + 1e-3)
            velocities += 0.5 * dt * accel
            positions += dt * velocities
            
            positions += beta * np.random.normal(0, 0.1, positions.shape) * dt
            for i, pos in enumerate(positions):
                cell_x = int(pos[0] / cell_size) % 128
                cell_y = int(pos[1] / cell_size) % 128
                cell_z = int(pos[2] / cell_size) % 128
                local_density = density_field[cell_x, cell_y, cell_z]
                velocities[i] *= (1.0 + gamma * local_density)
                if local_density < 0.2:
                    positions[i] += b * np.random.normal(0, 1.0, 3) * dt
            if delta > 0:
                n_new = int(len(positions) * delta * dt * 10)
                if n_new > 0:
                    flat_density = density_field.flatten()
                    probs = flat_density / np.sum(flat_density)
                    cell_indices = np.random.choice(len(probs), size=n_new, p=probs)
                    cell_coords = np.unravel_index(cell_indices, (128, 128, 128))
                    new_positions = np.column_stack([
                        (cell_coords[0] + np.random.random(n_new)) * box_size / 128,
                        (cell_coords[1] + np.random.random(n_new)) * box_size / 128,
                        (cell_coords[2] + np.random.random(n_new)) * box_size / 128
                    ])
                    new_velocities = np.random.normal(0, 400, (n_new, 3)) * self.Omega_m**0.55 / 1.8
                    positions = np.vstack([positions, new_positions])
                    velocities = np.vstack([velocities, new_velocities])
            positions += nu * np.random.normal(0, 0.1, positions.shape) * dt
            phase = 2 * np.pi * step * dt
            positions += c * np.sin(phase) * np.random.normal(0, 0.5, positions.shape) * dt
            
            accel = np.zeros_like(positions)
            for i in range(len(positions)):
                for j in range(i + 1, len(positions)):
                    r_vec = positions[j] - positions[i]
                    r = np.linalg.norm(r_vec)
                    if r > 1e-3:
                        accel[i] += alpha * r_vec / (r**3 + 1e-3)
                        accel[j] -= alpha * r_vec / (r**3 + 1e-3)
            velocities += 0.5 * dt * accel
            
            positions = np.clip(positions, 0, box_size)
            if step % 5 == 0:
                print(f"   ‚úÖ √âtape {step}/{n_steps} compl√©t√©e")
        
        return positions, velocities

class CosmicDNAMCMCValidator(CosmicDNALeapfrogValidator):
    """
    Validateur avec optimisation MCMC des coefficients
    """
    
    def optimize_coefficients_mcmc(self, positions, velocities, r_bins, box_size, 
                                 n_walkers=32, n_steps=1000, burn_in=200):
        """Optimise les coefficients avec emcee"""
        print(f"üîÑ Optimisation MCMC: {n_walkers} marcheurs, {n_steps} pas...")
        
        def log_likelihood(params, pos, vel, r_bins, box_size):
            alpha, beta, gamma, delta, nu, b, c = params
            if any(p < 0 or p > 1e-15 for p in params):
                return -np.inf
            try:
                pos_evolved, vel_evolved = self.evolve_positions_leapfrog(pos, vel, box_size, 
                                                                        alpha=alpha, beta=beta, 
                                                                        gamma=gamma, delta=delta, 
                                                                        nu=nu, b=b, c=c)
                _, xi, _ = self.compute_correlation_function(pos_evolved, r_bins, box_size, 'redshift')
                interp_xi = interp1d(r_bins[:-1], xi, bounds_error=False, fill_value=0.0)
                xi_sim = interp_xi(self.sdss_s)
                residuals = (xi_sim - self.sdss_xi_s) / self.sdss_errors
                return -0.5 * np.sum(residuals**2)
            except:
                return -np.inf
        
        def log_prior(params):
            return 0.0 if all(1e-18 < p < 1e-16 for p in params) else -np.inf
        
        def log_posterior(params, pos, vel, r_bins, box_size):
            lp = log_prior(params)
            if not np.isfinite(lp):
                return -np.inf
            return lp + log_likelihood(params, pos, vel, r_bins, box_size)
        
        ndim = 7
        pos = np.random.uniform(1e-17, 1e-16, (n_walkers, ndim))
        sampler = emcee.EnsembleSampler(n_walkers, ndim, log_posterior, 
                                       args=(positions, velocities, r_bins, box_size))
        sampler.run_mcmc(pos, n_steps, progress=False)
        
        samples = sampler.get_chain(discard=burn_in, flat=True)
        best_params = np.median(samples, axis=0)
        
        print(f"‚úÖ Param√®tres optimis√©s: Œ±={best_params[0]:.2e}, Œ≤={best_params[1]:.2e}, "
              f"Œ≥={best_params[2]:.2e}, Œ¥={best_params[3]:.2e}, ŒΩ={best_params[4]:.2e}, "
              f"b={best_params[5]:.2e}, c={best_params[6]:.2e}")
        
        return best_params, samples
    
    def compute_covariance_matrix(self, positions, r_bins, box_size, n_bootstrap=500):
        """Calcule la matrice de covariance avec 500 r√©√©chantillons pour une meilleure robustesse"""
        print(f"üîÑ Calcul matrice de covariance ({n_bootstrap} r√©√©chantillons)...")
        xi_bootstrap = []
        for i in range(n_bootstrap):
            indices = np.random.choice(len(positions), len(positions), replace=True)
            r_centers, xi_boot, _ = self.compute_correlation_function(positions[indices], r_bins, box_size, 'redshift')
            interp_func = interp1d(r_centers, xi_boot, bounds_error=False, fill_value=0.0)
            xi_bootstrap.append(interp_func(self.sdss_s))
            if i % 100 == 0:
                print(f"   ‚úÖ {i}/{n_bootstrap} bootstrap r√©ussis")
        covariance = np.cov(np.array(xi_bootstrap).T) + np.eye(len(self.sdss_s)) * 1e-6
        return covariance
    
    def compute_clustering_metrics(self, positions, eps=5.0, min_samples=10):
        """Calcule le clustering avec DBSCAN"""
        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(positions)
        labels = clustering.labels_
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        sizes = [np.sum(labels == i) for i in range(n_clusters)]
        return {'n_clusters': n_clusters, 'sizes': sizes}
    
    def compute_fractal_dimension(self, positions, box_size, n_bins=20):
        """Calcule la dimension fractale"""
        scales = np.logspace(-1, np.log10(box_size/2), n_bins)
        counts = []
        for scale in scales:
            grid_size = int(box_size / scale)
            grid, _ = np.histogramdd(positions, bins=grid_size, range=[[0, box_size]]*3)
            counts.append(np.sum(grid > 0))
        log_counts = np.log(counts)
        log_scales = np.log(1/scales)
        slope, _, _, _ = stats.linregress(log_scales, log_counts)
        return slope
    
    def validate_against_sdss(self, r_calc, xi_calc, covariance=None):
        """Validation statistique contre SDSS"""
        interp_xi = interp1d(r_calc, xi_calc, bounds_error=False, fill_value=0.0)
        xi_at_sdss = interp_xi(self.sdss_s)
        residuals = xi_at_sdss - self.sdss_xi_s
        
        if covariance is None:
            covariance = np.diag(self.sdss_errors**2)
        
        chi2 = residuals.T @ pinv(covariance) @ residuals
        dof = len(self.sdss_s) - 1
        p_value = 1 - stats.chi2.cdf(chi2, dof)
        compatible = p_value > 0.05
        rms_residual = np.sqrt(np.mean((residuals / self.sdss_errors)**2))
        
        return {
            'chi2': chi2,
            'dof': dof,
            'p_value': p_value,
            'compatible': compatible,
            'rms_residual': rms_residual,
            'residuals': residuals
        }

class CosmicDNAIllustrisValidator(CosmicDNAMCMCValidator):
    """
    Validateur avec comparaison √† IllustrisTNG
    """
    
    def load_illustris_snapshot(self, base_path, snapnum=99):
        """Charge snapshot IllustrisTNG (simul√© avec donn√©es publi√©es)"""
        print(f"üîÑ Chargement snapshot IllustrisTNG {snapnum}...")
        illustris_r = np.array([0.1, 0.5, 1, 5, 10, 20, 50])
        illustris_xi = np.array([17.5, 7.2, 3.6, 1.1, 0.62, 0.23, 0.07])
        illustris_clusters = 45
        illustris_fractal = 2.5
        print(f"‚úÖ Snapshot IllustrisTNG charg√© (simul√©)")
        return illustris_r, illustris_xi, illustris_clusters, illustris_fractal
    
    def comprehensive_validation(self, positions, velocities, r_bins, box_size, 
                               bias=1.8, regimes=None):
        """Validation compl√®te avec comparaison IllustrisTNG"""
        print("üîç Validation compl√®te Cosmic DNA...")
        results = {}
        
        if regimes is None:
            regimes = [
                {'name': 'Monolith', 'alpha': 1e-16, 'beta': 1e-18, 'gamma': 1e-18, 
                 'delta': 1e-18, 'nu': 1e-18, 'b': 1e-18, 'c': 1e-18},
                {'name': 'Crystal', 'alpha': 1e-18, 'beta': 1e-16, 'gamma': 1e-18, 
                 'delta': 1e-18, 'nu': 1e-18, 'b': 1e-18, 'c': 1e-18},
                {'name': 'Balanced', 'alpha': 5e-17, 'beta': 5e-17, 'gamma': 5e-17, 
                 'delta': 5e-17, 'nu': 5e-17, 'b': 5e-17, 'c': 5e-17}
            ]
        
        density_field, _ = self._generate_density_field_with_gradients(128, box_size)
        
        for regime in regimes:
            print(f"\nüîç Analyse r√©gime {regime['name']}...")
            pos_evolved, vel_evolved = self.evolve_positions_leapfrog(
                positions, velocities, box_size,
                alpha=regime['alpha'], beta=regime['beta'], gamma=regime['gamma'],
                delta=regime['delta'], nu=regime['nu'], b=regime['b'], c=regime['c']
            )
            redshift_pos = self.apply_redshift_space_distortions(pos_evolved, vel_evolved)
            r_redshift, xi_redshift, _ = self.compute_correlation_function(redshift_pos, r_bins, box_size, 'redshift')
            covariance = self.compute_covariance_matrix(redshift_pos, r_bins, box_size)
            validation = self.validate_against_sdss(r_redshift, xi_redshift, covariance)
            k_centers, Pk = self.compute_power_spectrum(redshift_pos, box_size)
            clustering = self.compute_clustering_metrics(redshift_pos)
            fractal_dim = self.compute_fractal_dimension(redshift_pos, box_size)
            
            results[regime['name']] = {
                'positions': redshift_pos,
                'velocities': vel_evolved,
                'redshift_space': {'r': r_redshift, 'xi': xi_redshift, 'validation': validation},
                'power_spectrum': {'k': k_centers, 'Pk': Pk},
                'clustering': clustering,
                'fractal_dimension': fractal_dim
            }
        
        print("\nüîç Optimisation MCMC...")
        best_params, mcmc_samples = self.optimize_coefficients_mcmc(positions, velocities, r_bins, box_size)
        pos_mcmc, vel_mcmc = self.evolve_positions_leapfrog(
            positions, velocities, box_size,
            alpha=best_params[0], beta=best_params[1], gamma=best_params[2],
            delta=best_params[3], nu=best_params[4], b=best_params[5], c=best_params[6]
        )
        redshift_pos_mcmc = self.apply_redshift_space_distortions(pos_mcmc, vel_mcmc)
        r_redshift_mcmc, xi_redshift_mcmc, _ = self.compute_correlation_function(redshift_pos_mcmc, r_bins, box_size, 'redshift')
        covariance_mcmc = self.compute_covariance_matrix(redshift_pos_mcmc, r_bins, box_size)
        validation_mcmc = self.validate_against_sdss(r_redshift_mcmc, xi_redshift_mcmc, covariance_mcmc)
        k_centers_mcmc, Pk_mcmc = self.compute_power_spectrum(redshift_pos_mcmc, box_size)
        clustering_mcmc = self.compute_clustering_metrics(redshift_pos_mcmc)
        fractal_dim_mcmc = self.compute_fractal_dimension(redshift_pos_mcmc, box_size)
        
        results['MCMC'] = {
            'positions': redshift_pos_mcmc,
            'velocities': vel_mcmc,
            'redshift_space': {'r': r_redshift_mcmc, 'xi': xi_redshift_mcmc, 'validation': validation_mcmc},
            'power_spectrum': {'k': k_centers_mcmc, 'Pk': Pk_mcmc},
            'clustering': clustering_mcmc,
            'fractal_dimension': fractal_dim_mcmc
        }
        
        print("\nüîç Comparaison avec IllustrisTNG...")
        illustris_r, illustris_xi, illustris_clusters, illustris_fractal = self.load_illustris_snapshot('./TNG100/output')
        results['illustris'] = {
            'r': illustris_r, 'xi': illustris_xi, 'n_clusters': illustris_clusters, 
            'fractal_dimension': illustris_fractal
        }
        
        return results, best_params, mcmc_samples

class CosmicDNAMetricsValidator(CosmicDNAIllustrisValidator):
    """
    Validateur avec m√©triques suppl√©mentaires (rapport vide-amas, p√©riodicit√©)
    """
    
    def compute_void_cluster_ratio(self, density_field, threshold_low=0.2, threshold_high=0.8):
        """Rapport vide-amas"""
        void_volume = np.sum(density_field < threshold_low) / density_field.size
        cluster_volume = np.sum(density_field > threshold_high) / density_field.size
        return void_volume / (cluster_volume + 1e-6)
    
    def compute_periodicity(self, positions, n_freq=10):
        """P√©riodicit√© via FFT"""
        fft_x = np.fft.fftn(positions[:, 0])
        amp_x = np.abs(fft_x)
        return np.max(amp_x[:n_freq])
    
    def comprehensive_validation(self, positions, velocities, r_bins, box_size, 
                               bias=1.8, regimes=None):
        """Validation compl√®te avec m√©triques suppl√©mentaires"""
        results, best_params, mcmc_samples = super().comprehensive_validation(positions, velocities, r_bins, box_size, bias, regimes)
        density_field, _ = self._generate_density_field_with_gradients(128, box_size)
        for regime_name, data in results.items():
            if regime_name == 'illustris':
                continue
            data['void_cluster_ratio'] = self.compute_void_cluster_ratio(density_field)
            data['periodicity'] = self.compute_periodicity(data['positions'])
        return results, best_params, mcmc_samples
